#!/usr/bin/env python3
"""
Fine-tune a BERT encoder (LogBERT-style) on the prepared Spark job dataset.

The script expects JSON Lines files generated by `prepare_logbert_dataset.py`
and trains a binary sequence classifier that distinguishes successful jobs
from failed ones. It relies on Hugging Face Transformers + PyTorch.
"""

from __future__ import annotations

import argparse
import inspect
import json
import logging
from collections import Counter
from pathlib import Path
from typing import Dict, List

import torch
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from torch.utils.data import Dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    Trainer,
    TrainingArguments,
    set_seed,
)


LOGGER = logging.getLogger("logbert-train")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train a LogBERT-style classifier.")
    parser.add_argument(
        "--train-data",
        type=Path,
        default=Path("logbert_pipeline/data/train.jsonl"),
        help="Path to training JSONL file produced by prepare_logbert_dataset.py.",
    )
    parser.add_argument(
        "--eval-data",
        type=Path,
        default=Path("logbert_pipeline/data/test.jsonl"),
        help="Path to evaluation JSONL file (same format as train).",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("logbert_pipeline/model"),
        help="Directory to store checkpoints and the final model.",
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="bert-base-uncased",
        help="Hugging Face model checkpoint to fine-tune.",
    )
    parser.add_argument(
        "--max-length",
        type=int,
        default=512,
        help="Maximum token length per sequence (after tokenization).",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="Per-device batch size for training and evaluation.",
    )
    parser.add_argument(
        "--epochs",
        type=float,
        default=5.0,
        help="Number of training epochs.",
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=5e-5,
        help="AdamW learning rate.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility.",
    )
    parser.add_argument(
        "--warmup-ratio",
        type=float,
        default=0.06,
        help="Warm-up ratio for learning rate scheduler.",
    )
    parser.add_argument(
        "--weight-decay",
        type=float,
        default=0.01,
        help="Weight decay to apply during optimization.",
    )
    parser.add_argument(
        "--class-balance",
        action="store_true",
        help="Apply inverse-frequency class weights to the loss to counter label imbalance.",
    )
    return parser.parse_args()


def load_jsonl(path: Path) -> List[Dict[str, object]]:
    if not path.exists():
        raise FileNotFoundError(f"Dataset file not found: {path}")
    entries: List[Dict[str, object]] = []
    with path.open("r", encoding="utf-8") as handle:
        for line in handle:
            if line.strip():
                entries.append(json.loads(line))
    if not entries:
        raise ValueError(f"No entries loaded from {path}")
    return entries


def events_to_text(events: List[str]) -> str:
    """Join templates with separators to mimic log token sequences."""
    return " [SEP] ".join(event.strip() for event in events if event)


class LogDataset(Dataset):
    def __init__(self, entries: List[Dict[str, object]], tokenizer, max_length: int):
        texts = [events_to_text(e["events"]) for e in entries]
        labels = [int(e["label"]) for e in entries]

        self.encodings = tokenizer(
            texts,
            truncation=True,
            padding=False,
            max_length=max_length,
        )
        self.labels = labels

    def __len__(self) -> int:
        return len(self.labels)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item


def compute_metrics(pred) -> Dict[str, float]:
    logits = pred.predictions
    preds = logits.argmax(axis=-1)
    labels = pred.label_ids

    acc = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average="binary", zero_division=0
    )
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}


def configure_logging() -> None:
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        level=logging.INFO,
    )


def main() -> None:
    args = parse_args()
    configure_logging()
    set_seed(args.seed)

    args.output_dir.mkdir(parents=True, exist_ok=True)

    LOGGER.info("Loading datasets …")
    train_entries = load_jsonl(args.train_data)
    eval_entries = load_jsonl(args.eval_data)

    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_name)
        num_labels = 2
        model = AutoModelForSequenceClassification.from_pretrained(
            args.model_name, num_labels=num_labels
        )
    except OSError as exc:
        LOGGER.error(
            "Failed to load model/tokenizer '%s'. If you are working offline, "
            "download the checkpoint first and pass its local path via --model-name.",
            args.model_name,
        )
        raise

    train_dataset = LogDataset(train_entries, tokenizer, args.max_length)
    eval_dataset = LogDataset(eval_entries, tokenizer, args.max_length)

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    class_weights_tensor: torch.Tensor | None = None
    if args.class_balance:
        label_counts = Counter(train_dataset.labels)
        if len(label_counts) < 2 or min(label_counts.values()) == 0:
            LOGGER.warning(
                "Class balancing requested, but at least one class is missing in the training data. Skipping weighting."
            )
        else:
            num_classes = len(label_counts)
            total = sum(label_counts.values())
            weights = []
            for class_id in range(num_classes):
                count = label_counts.get(class_id, 0)
                weight = 0.0 if count == 0 else total / (num_classes * count)
                weights.append(weight)
            class_weights_tensor = torch.tensor(weights, dtype=torch.float)
            LOGGER.info(
                "Applying class weights to loss (counts=%s, weights=%s)",
                dict(label_counts),
                [round(w, 4) for w in class_weights_tensor.tolist()],
            )

    logging_steps = max(1, len(train_dataset) // args.batch_size // 2)

    training_kwargs = dict(
        output_dir=str(args.output_dir),
        overwrite_output_dir=True,
        save_strategy="epoch",
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        num_train_epochs=args.epochs,
        weight_decay=args.weight_decay,
        warmup_ratio=args.warmup_ratio,
        logging_steps=logging_steps,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        report_to="none",
        seed=args.seed,
    )

    # `eval_strategy` replaced `evaluation_strategy` in newer transformers releases.
    training_args_params = inspect.signature(TrainingArguments.__init__).parameters
    if "evaluation_strategy" in training_args_params:
        training_kwargs["evaluation_strategy"] = "epoch"
    else:
        training_kwargs["eval_strategy"] = "epoch"

    training_args = TrainingArguments(**training_kwargs)

    class WeightedTrainer(Trainer):
        def __init__(self, *args, class_weights=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.class_weights = class_weights

        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
            outputs = model(**inputs)
            labels = inputs["labels"]

            if isinstance(outputs, dict):
                logits = outputs["logits"]
                base_loss = outputs.get("loss")
            else:
                logits = outputs.logits
                base_loss = getattr(outputs, "loss", None)

            module = getattr(model, "module", model)
            num_labels = module.config.num_labels if hasattr(module, "config") else logits.shape[-1]

            if self.class_weights is not None:
                weight = self.class_weights.to(logits.device)
                loss_fct = torch.nn.CrossEntropyLoss(weight=weight)
                loss = loss_fct(
                    logits.view(-1, num_labels),
                    labels.view(-1),
                )
            else:
                loss = base_loss
                if loss is None:
                    loss = F.cross_entropy(
                        logits.view(-1, num_labels),
                        labels.view(-1),
                    )

            return (loss, outputs) if return_outputs else loss

    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        class_weights=class_weights_tensor,
    )

    LOGGER.info("Starting training …")
    trainer.train()

    LOGGER.info("Evaluating best checkpoint …")
    metrics = trainer.evaluate()
    for key, value in metrics.items():
        LOGGER.info("  %s = %.4f", key, value)

    LOGGER.info("Saving model to %s", args.output_dir)
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)


if __name__ == "__main__":
    main()
